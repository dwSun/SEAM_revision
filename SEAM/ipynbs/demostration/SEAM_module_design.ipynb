{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demostrate how SEAM package is designed. The functions within the notebook are arranged to different modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SEAM.datasets\n",
    "\n",
    "# raw dataset/processed dataset\n",
    "import SEAM.tl.Cut\n",
    "\n",
    "# load segmentation result, and get anndata, prepare obs/obsm, e.g. spatial...\n",
    "import SEAM.tl.ID\n",
    "\n",
    "# run ID and add ID representation to adata\n",
    "import SEAM.tl.Umap\n",
    "\n",
    "# run UMAP using SIMSID/mean\n",
    "import SEAM.tl.Cluster\n",
    "\n",
    "# run clustering algorithms, updata obs cluster of anndata\n",
    "import SEAM.tl.Diff\n",
    "\n",
    "# run differential methods, either SIMS-Diff or single cell methods, update anndata\n",
    "import SEAM.tl.View\n",
    "\n",
    "# run SIMS-View, update anndata, user can select color space and DM methods\n",
    "import SEAM.tl.ME\n",
    "\n",
    "# input list of points to form polylines, update anndata as boxplot/lineplot associated variables.\n",
    "\n",
    "\n",
    "import SEAM.pl.IMS\n",
    "\n",
    "# raw data ploting\n",
    "import SEAM.pl.Cut\n",
    "\n",
    "# ploting cut result, dot, real, or overlay of real/134\n",
    "import SEAM.pl.ID\n",
    "\n",
    "# ploting umap/PCA/Tsne of ID\n",
    "import SEAM.pl.Cluster\n",
    "\n",
    "# ploting spatial single cell map dot/real\n",
    "import SEAM.pl.Diff\n",
    "\n",
    "# plot heatmap given a set of genes\n",
    "import SEAM.pl.View\n",
    "\n",
    "# plot SIMS-View\n",
    "import SEAM.pl.ME\n",
    "\n",
    "# plot polyline overlay with cut/clustering, plot boxplot/lineplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import palettable\n",
    "\n",
    "sc.set_figure_params(dpi=500, color_map=\"viridis\", dpi_save=500, transparent=True)\n",
    "\n",
    "sc.settings.verbosity = 2\n",
    "heatmap_cmp = palettable.cmocean.diverging.Balance_20.mpl_colormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "\n",
    "DATA_PATH_IMS_PROCESSED = \"/home/yzy/SEAM/data/process/\"\n",
    "DATA_PATH_DUMP = \"/home/yzy/SEAM/data/dump/\"\n",
    "\n",
    "\n",
    "def get_train_data(data_mat_filename, mode, norm, batch_num_list=[1]):\n",
    "\n",
    "    original_data = {}\n",
    "    cell_related_data = {}\n",
    "    data_mat = sio.loadmat(data_mat_filename)\n",
    "    data_mat = data_mat[\"data_mat\"]\n",
    "\n",
    "    num_features = data_mat.shape[1] - 3\n",
    "    batch_dict = {}\n",
    "\n",
    "    label_dict = {}\n",
    "    cell_dict = {}\n",
    "    pos_dict = {}\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "        cur_data = data_mat[data_mat[:, 0] == i + 1, 3 : num_features + 3]\n",
    "\n",
    "        batch_dict[i + 1] = cur_data\n",
    "\n",
    "        cell_dict[i + 1] = data_mat[data_mat[:, 0] == i + 1, 1]\n",
    "        cur_batch_idx = data_mat[data_mat[:, 0] == i + 1, 2]\n",
    "        label_dict[i + 1] = np.ones(shape=cur_batch_idx.shape)\n",
    "\n",
    "        pos_dict[i + 1] = cur_batch_idx\n",
    "    original_data[\"batch_dict\"] = batch_dict\n",
    "    original_data[\"cell_dict\"] = cell_dict\n",
    "    original_data[\"label_dict\"] = label_dict\n",
    "    original_data[\"pos_dict\"] = pos_dict\n",
    "\n",
    "    top_n_var = 250\n",
    "    train_x_all = None\n",
    "    cell_idx_all = None\n",
    "    cell_type_all = None\n",
    "    cell_pos_all = None\n",
    "    batch_idx_all = None\n",
    "    num_cells_all = 0\n",
    "\n",
    "    for batch_num in batch_num_list:\n",
    "        train_x = batch_dict[batch_num]\n",
    "        # train_x = eval('batch_dict_{norm_type}[batch_num]'.format(norm_type=norm_type))\n",
    "        # train_x = batch_dict[batch_num]\n",
    "        cell_idx = cell_dict[batch_num]\n",
    "        cell_type = label_dict[batch_num]\n",
    "        cell_pos = pos_dict[batch_num]\n",
    "        # batch_FE = FE_dict[batch_num]\n",
    "        cell_related_ind = cell_idx != 0\n",
    "\n",
    "        num_cells = int(np.max(cell_idx))\n",
    "        # num_cells = 2\n",
    "        train_x = train_x[cell_related_ind, :]\n",
    "\n",
    "        cell_idx = cell_idx[cell_related_ind]\n",
    "        cell_type = cell_type[cell_related_ind]\n",
    "        # cell_type = np.ones(shape=cell_idx.shape)\n",
    "        cell_pos = cell_pos[cell_related_ind]\n",
    "\n",
    "        var_li = []\n",
    "        normed_var_li = []\n",
    "        for i in range(train_x.shape[1]):\n",
    "            cur_col = train_x[:, i]\n",
    "            #     cur_col= cur_row/np.sum(cur_col)\n",
    "            #     cur_entropy = entropy(cur_col)\n",
    "            cur_var = np.var(cur_col)\n",
    "            cur_normed_var = cur_var / np.mean(cur_col)\n",
    "            #     entropy_li.append(cur_entropy)\n",
    "            var_li.append(cur_var)\n",
    "            normed_var_li.append(cur_normed_var)\n",
    "        # entropy_li = np.array(entropy_li)\n",
    "        var_li = np.array(var_li)\n",
    "        normed_var_li = np.array(normed_var_li)\n",
    "        sort_ind = np.flip(np.argsort(normed_var_li), axis=0)\n",
    "        sort_val = np.flip(np.sort(normed_var_li), axis=0)\n",
    "\n",
    "        if train_x_all is None:\n",
    "            train_x_all = train_x\n",
    "        else:\n",
    "            train_x_all = np.vstack([train_x_all, train_x])\n",
    "        if cell_idx_all is None:\n",
    "            cell_idx_all = cell_idx\n",
    "        else:\n",
    "            cell_idx_all = np.hstack([cell_idx_all, cell_idx + np.max(cell_idx_all)])\n",
    "        if cell_type_all is None:\n",
    "            cell_type_all = cell_type\n",
    "        else:\n",
    "            cell_type_all = np.hstack([cell_type_all, cell_type])\n",
    "        if cell_pos_all is None:\n",
    "            cell_pos_all = cell_pos\n",
    "        else:\n",
    "            cell_pos_all = np.hstack([cell_pos_all, cell_pos])\n",
    "        if batch_idx_all is None:\n",
    "            batch_idx_all = batch_num * np.ones(shape=(cell_idx.shape))\n",
    "        else:\n",
    "            batch_idx_all = np.hstack([batch_idx_all, batch_num * np.ones(shape=(cell_idx.shape))])\n",
    "\n",
    "    train_x = train_x_all\n",
    "    cell_idx = cell_idx_all\n",
    "    cell_type = cell_type_all\n",
    "    cell_pos = cell_pos_all\n",
    "    batch_idx = batch_idx_all\n",
    "    num_cells = np.max(cell_idx)\n",
    "    if mode == \"none\":\n",
    "        train_x = train_x\n",
    "    elif mode == \"median\":\n",
    "        train_x = train_x / np.percentile(train_x, 50, axis=1, keepdims=True)\n",
    "        train_x = np.log(train_x + 1)\n",
    "    elif mode == \"total\":\n",
    "        train_x = train_x / np.sum(train_x, axis=1, keepdims=True)\n",
    "        train_x = np.log(train_x + 1)\n",
    "\n",
    "    if norm == \"standard\":\n",
    "        train_x = StandardScaler().fit_transform(train_x)\n",
    "    elif norm == \"l1\":\n",
    "        train_x = Normalizer(norm=\"l1\").fit_transform(train_x)\n",
    "    elif norm == \"l2\":\n",
    "        train_x = Normalizer(norm=\"l2\").fit_transform(train_x)\n",
    "    elif norm == \"none\":\n",
    "        train_x = train_x\n",
    "\n",
    "    cell_related_data[\"train_x\"] = train_x\n",
    "    cell_related_data[\"cell_idx\"] = cell_idx\n",
    "    cell_related_data[\"cell_type\"] = cell_type\n",
    "    cell_related_data[\"cell_pos\"] = cell_pos\n",
    "    cell_related_data[\"batch_idx\"] = batch_idx\n",
    "    cell_related_data[\"num_cells\"] = num_cells\n",
    "    return original_data, cell_related_data\n",
    "\n",
    "\n",
    "def load_raw_SIMS(data):\n",
    "    data_mat_filename_temp = DATA_PATH_IMS_PROCESSED + \"{0}/cut/rst/datamat.mat\"\n",
    "    matter_list_filename_temp = DATA_PATH_IMS_PROCESSED + \"{0}/preprocess/matters_candidate.pkl\"\n",
    "\n",
    "    #     data = 'P6_neg1_low0_None_auto'\n",
    "    test_sample_temp = DATA_PATH_IMS_PROCESSED + \"{0}/preprocess/test_samples.mat\"\n",
    "\n",
    "    matter_list_filename = matter_list_filename_temp.format(data)\n",
    "    data_mat_filename = data_mat_filename_temp.format(data)\n",
    "    test_sample_filename = test_sample_temp.format(data)\n",
    "    test_sample_all = sio.loadmat(test_sample_filename)[\"test_samples\"]\n",
    "    mode = \"none\"\n",
    "    norm = \"none\"\n",
    "    [original_data, cell_related_data] = get_train_data(data_mat_filename, mode, norm, batch_num_list=[1])\n",
    "    train_x = cell_related_data[\"train_x\"]\n",
    "    cell_idx = cell_related_data[\"cell_idx\"]\n",
    "    cell_pos = cell_related_data[\"cell_pos\"]\n",
    "    num_cells = np.max(cell_idx)\n",
    "    matter_list = pickle.load(open(matter_list_filename, \"rb\"))\n",
    "    matter_list = np.array(matter_list)\n",
    "\n",
    "    return train_x, cell_idx, cell_pos, matter_list, num_cells, test_sample_all\n",
    "\n",
    "\n",
    "def get_mean_representation(train_x, cell_idx, num_cells):\n",
    "    train_x_tmp = train_x.copy()\n",
    "    train_x_median = (train_x_tmp + 1) / (np.percentile(train_x_tmp, 50, axis=1, keepdims=True) + 1)\n",
    "    train_x_total = train_x / np.sum(train_x, axis=1, keepdims=True)\n",
    "    train_x_median = np.log(train_x_median + 1)\n",
    "    train_x_total = np.log(train_x_total + 1)\n",
    "    # train_x_A = (train_x+1)/(train_x[:,matter_list==134.06]+1)\n",
    "    sum_profile_list_median = []\n",
    "    sum_profile_list_total = []\n",
    "\n",
    "    max_profile_list_median = []\n",
    "    max_profile_list = []\n",
    "    mean_profile_list_median = []\n",
    "    max_profile_list_total = []\n",
    "    mean_profile_list_total = []\n",
    "    mean_profile_list = []\n",
    "    # mean_profile_list_A=[]\n",
    "    # max_profile_list_A = []\n",
    "    for i in range(num_cells):\n",
    "        mean_profile_list_median.append(np.mean(train_x_median[cell_idx == i + 1, :], axis=0))\n",
    "        max_profile_list_median.append(np.max(train_x_median[cell_idx == i + 1, :], axis=0))\n",
    "        max_profile_list.append(np.max(train_x[cell_idx == i + 1, :], axis=0))\n",
    "        #         mean_profile_list_A.append(np.mean(train_x_A[cell_idx==i+1,:],axis=0))\n",
    "        #         max_profile_list_A.append(np.max(train_x_A[cell_idx==i+1,:],axis=0))\n",
    "\n",
    "        sum_profile_list_median.append(np.sum(train_x_median[cell_idx == i + 1, :], axis=0))\n",
    "        mean_profile_list.append(np.mean(train_x[cell_idx == i + 1, :], axis=0))\n",
    "        mean_profile_list_total.append(np.mean(train_x_total[cell_idx == i + 1, :], axis=0))\n",
    "        max_profile_list_total.append(np.max(train_x_total[cell_idx == i + 1, :], axis=0))\n",
    "        sum_profile_list_total.append(np.sum(train_x_total[cell_idx == i + 1, :], axis=0))\n",
    "\n",
    "    mean_profile_list_median = np.array(mean_profile_list_median)\n",
    "    max_profile_list_median = np.array(max_profile_list_median)\n",
    "    mean_profile_list_total = np.array(mean_profile_list_total)\n",
    "    max_profile_list_total = np.array(max_profile_list_total)\n",
    "    sum_profile_list_median = np.array(sum_profile_list_median)\n",
    "    sum_profile_list_total = np.array(sum_profile_list_total)\n",
    "    mean_profile_list = np.array(mean_profile_list)\n",
    "    max_profile_list = np.array(max_profile_list)\n",
    "    # mean_profile_list_A = np.array(mean_profile_list_A)\n",
    "    # max_profile_list_A = np.array(max_profile_list_A)\n",
    "    return mean_profile_list_median\n",
    "\n",
    "\n",
    "def load_dataset_raw(data):\n",
    "    train_x, cell_idx, cell_pos, matter_list, num_cells, test_sample_all = load_raw_SIMS(data)\n",
    "    mean_profile_list_median = get_mean_representation(train_x, cell_idx, num_cells)\n",
    "\n",
    "    in_X = mean_profile_list_median\n",
    "    # g = map(str,range(in_X.shape[1]))\n",
    "    g = map(str, matter_list)\n",
    "    Genes = []\n",
    "    None_idx = 0\n",
    "    Genes = g\n",
    "    # obs_name must be str\n",
    "    obs_name = list(map(str, range(in_X.shape[0])))\n",
    "    obs = pd.DataFrame(index=obs_name)\n",
    "\n",
    "    # var_name must be str\n",
    "    var = pd.DataFrame(index=Genes)\n",
    "\n",
    "    #     var['Genes'] = Genes\n",
    "    a = ad.AnnData(in_X, obs=obs, var=var, dtype=\"float32\")\n",
    "    a.uns[\"cell_idx\"] = cell_idx\n",
    "    a.uns[\"cell_pos\"] = cell_pos\n",
    "    a.uns[\"IMS\"] = test_sample_all\n",
    "    a.uns[\"train_x\"] = train_x\n",
    "    #     a.uns['test_sample_all'] =\n",
    "    #     a.uns['rep_list'] = rep_list\n",
    "    return a\n",
    "\n",
    "\n",
    "def load_dataset_processed(data):\n",
    "    data_dump_path = DATA_PATH_DUMP + \"{0}/data.h5ad\".format(data)\n",
    "\n",
    "    a = ad.read_h5ad(data_dump_path)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_dataset_raw(\"P6_neg1_low0_None_auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.uns[\"train_x\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_dataset_processed(\"P1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_matters(key_matters, matter_list):\n",
    "    key_matters = np.array(key_matters).astype(\"float\")\n",
    "    matter_list = np.array(matter_list).astype(\"float\")\n",
    "    adjested_key_matters = []\n",
    "    for k in key_matters:\n",
    "        m_diff = np.abs(matter_list - k)\n",
    "        min_diff_idx = np.argmin(m_diff)\n",
    "        min_diff_m = matter_list[min_diff_idx]\n",
    "        adjested_key_matters.append(min_diff_m)\n",
    "    adjested_key_matters = np.array(adjested_key_matters)\n",
    "    return adjested_key_matters\n",
    "\n",
    "\n",
    "def add_subcls(a_cls_sub, ec_cls, to_replace_cls):\n",
    "    a_cls_sub_int = a_cls_sub.astype(\"int\")\n",
    "    a_cls_sub_int[a_cls_sub_int == int(to_replace_cls)] = -1\n",
    "    a_cls_sub_int[a_cls_sub_int > int(to_replace_cls)] -= 1\n",
    "    a_cls_sub_int_max = a_cls_sub_int.max()\n",
    "\n",
    "    ec_cls_int = ec_cls.astype(\"int\")\n",
    "    ec_cls_int -= ec_cls_int.min()\n",
    "    ec_cls_int += a_cls_sub_int_max + 1\n",
    "    a_cls_sub_int[a_cls_sub_int == -1] = ec_cls_int\n",
    "    return a_cls_sub_int.astype(\"str\")\n",
    "\n",
    "\n",
    "def ind2ij(ind, size, axis):\n",
    "    i, j = divmod(ind - 1, size)\n",
    "    i += 1\n",
    "    j += 1\n",
    "    return np.array([i, j])[axis]\n",
    "\n",
    "\n",
    "def get_labeling(label, cell_idx, cell_pos):\n",
    "    #     y是cell-rela的细胞对应的标签\n",
    "    #     print('pred_y',np.unique(label))\n",
    "    labeling = np.zeros(shape=(65536, 1))\n",
    "    b = cell_idx.copy()\n",
    "    num_cells = label.shape[0]\n",
    "    for i in range(num_cells):\n",
    "        b[b == i + 1] = label[i] + 1\n",
    "    #     print(cell_pos)\n",
    "    #     print('b',np.unique(b))\n",
    "    #     cell_pos = cell_pos.astype('int')\n",
    "    labeling[cell_pos.astype(\"int\") - 1, 0] = b\n",
    "\n",
    "    return labeling\n",
    "\n",
    "\n",
    "def plot_label_image(a, pred_y, cmp, save=None, mask=None, figsize=(5, 5), anno=False, ifshow=True):\n",
    "\n",
    "    cell_idx = a.uns[\"cell_idx\"]\n",
    "    cell_pos = a.uns[\"cell_pos\"]\n",
    "\n",
    "    to_labeling_pred_y = np.array(pred_y.astype(\"int\"))\n",
    "    to_labeling_pred_y_min = to_labeling_pred_y.min()\n",
    "    # to_labeling_pred_y[coc[448,:]>0]=3\n",
    "    # to_labeling_pred_y = resultsLWEA[:,2]\n",
    "    # to_labeling_pred_y = label_list_FF[2]\n",
    "    # to_labeling_pred_y = label_list[3]\n",
    "    # to_labeling_pred_y[mark_list]=2\n",
    "    #     cluster_cmp = sns.hls_palette(np.unique(to_labeling_pred_y).shape[0])\n",
    "    unique_cls = np.unique(pred_y).shape[0]\n",
    "    #     unique_cls_mask = [unique_cls[m] for m in mask]\n",
    "    cluster_cmp = cmp.copy()\n",
    "\n",
    "    if mask is not None:\n",
    "        for to_mask in range(unique_cls):\n",
    "            if to_mask in mask:\n",
    "                continue\n",
    "            cluster_cmp[to_mask] = \"k\"\n",
    "    labeling_plot_cmp = [\"k\"]\n",
    "    labeling_plot_cmp.extend(cluster_cmp)\n",
    "    labeling = get_labeling(to_labeling_pred_y - np.min(to_labeling_pred_y), cell_idx, cell_pos)\n",
    "    # labeling[labeling==5]=0\n",
    "    img1 = labeling.reshape((256, 256))\n",
    "    plt.figure(figsize=figsize)\n",
    "    # plt.imshow(img1)\n",
    "    ticks = np.arange(np.min(img1) + 1, np.max(img1) + 1)\n",
    "    boundaries = np.arange(np.min(img1) + 0.5, np.max(img1) + 1.5)\n",
    "    #     with sns.plotting_context(font_scale=font_scale):\n",
    "    sns.heatmap(\n",
    "        img1,\n",
    "        cmap=labeling_plot_cmp,\n",
    "        linewidths=0,\n",
    "        linecolor=\"k\",\n",
    "        square=True,\n",
    "        cbar_kws={\"ticks\": ticks, \"boundaries\": boundaries, \"fraction\": 0.046, \"pad\": 0.04},\n",
    "    )\n",
    "    # sns.heatmap(img1,cmap=labeling_plot_cmp,square=True,ad':0.04})\n",
    "    #     plt.legend(fontsize=font_size)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if save is not None:\n",
    "        plt.savefig(save, transparent=False, format=\"png\", bbox_inches=\"tight\")\n",
    "\n",
    "    if anno:\n",
    "        num_cells = pred_y.shape[0]\n",
    "        for i in range(num_cells):\n",
    "            cur_idx = i + 1\n",
    "            cur_ind = cell_pos[cell_idx == cur_idx][0]\n",
    "            #     print(ind2ij(cur_ind,256,0))\n",
    "            #     print(ind2ij(cur_ind,256,1))\n",
    "            if to_labeling_pred_y[i] - to_labeling_pred_y_min in mask:\n",
    "                plt.annotate(str(cur_idx - 1), (ind2ij(cur_ind, 256, 1), ind2ij(cur_ind, 256, 0)), color=\"red\")\n",
    "\n",
    "    if ifshow:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Cut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(a):\n",
    "    num_cells = a.shape[0]\n",
    "    cell_idx = a.uns[\"cell_idx\"]\n",
    "    cell_pos = a.uns[\"cell_pos\"]\n",
    "    cell_pos_list = []\n",
    "\n",
    "    for i in range(num_cells):\n",
    "        cur_idx = i + 1\n",
    "        cur_ind = cell_pos[cell_idx == cur_idx][0]\n",
    "        cur_ind_list = cell_pos[cell_idx == cur_idx]\n",
    "        cur_x_list = []\n",
    "        cur_y_list = []\n",
    "        for j in cur_ind_list:\n",
    "            cur_x = ind2ij(cur_ind, 256, 1)\n",
    "            cur_y = ind2ij(cur_ind, 256, 0)\n",
    "            cur_x_list.append(cur_x)\n",
    "            cur_y_list.append(cur_y)\n",
    "        cur_x_mean = np.mean(cur_x_list)\n",
    "        cur_y_mean = np.mean(cur_y_list)\n",
    "\n",
    "        cell_pos_list.append(np.array([cur_x_mean, cur_y_mean]))\n",
    "\n",
    "    cell_pos_mat = np.array(cell_pos_list)\n",
    "    print(\"setting obsm: spatial\")\n",
    "    a.obsm[\"spatial\"] = cell_pos_mat\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Cut(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "sc.pl.embedding(a, basis=\"spatial\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Umap(a, rep=\"ID\"):\n",
    "    if rep == \"ID\":\n",
    "        sc.pp.neighbors(a, use_rep=\"ID\", metric=\"cosine\", n_neighbors=15)\n",
    "    else:\n",
    "        sc.pp.neighbors(a, n_neighbors=15)\n",
    "    sc.tl.umap(a)\n",
    "    print(\"Sucessfully run Umap!\")\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.constraints import *\n",
    "from keras.regularizers import *\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "\n",
    "\n",
    "def get_distil_rep(\n",
    "    train_x,\n",
    "    cell_idx,\n",
    "    num_cells,\n",
    "    t_list,\n",
    "    epochs=50,\n",
    "    verbose=False,\n",
    "    activa=\"relu\",\n",
    "    dp_rate=0.5,\n",
    "    low_dim=128,\n",
    "    l2_penalty=0,\n",
    "    l1_penalty=1e-5,\n",
    "    use_bias=False,\n",
    "    netwidths=[512, 256, 128],\n",
    "    error_threshold=2,\n",
    "):\n",
    "\n",
    "    SIMS_input = Input(shape=(train_x.shape[1],))\n",
    "    target_input = Input(shape=(1,))\n",
    "    kernel_init_func = glorot_normal()\n",
    "\n",
    "    d1 = Dense(netwidths[0], activation=activa, kernel_initializer=kernel_init_func, kernel_regularizer=l2(l2_penalty), use_bias=use_bias)(SIMS_input)\n",
    "    # d1 = Dense(1024,activation=activa,kernel_initializer='random_uniform')(SIMS_input)\n",
    "    # d1 = Dense(1024,activation=activa,kernel_initializer='random_uniform')(d1)\n",
    "\n",
    "    d2 = Dense(netwidths[1], activation=activa, kernel_initializer=kernel_init_func, kernel_regularizer=l2(l2_penalty), use_bias=use_bias)(d1)\n",
    "    # d2 = Dense(512,activation=activa,kernel_initializer='random_uniform')(d1)\n",
    "    # d2 = Dense(512,activation=activa,kernel_initializer='random_uniform')(d2)\n",
    "    d2 = Dense(netwidths[2], activation=\"linear\", kernel_initializer=kernel_init_func, use_bias=use_bias)(d2)\n",
    "\n",
    "    # d3 = Dense(64,activation=activa,kernel_initializer=glorot_normal(),kernel_regularizer=l2(l2_penalty))(d2)\n",
    "    # d3 = Dense(256,activation=activa,kernel_initializer='random_uniform')(d2)\n",
    "    # d3 = Dense(256,activation=activa,kernel_initializer='random_uniform')(d3)\n",
    "\n",
    "    # d4 = Dense(low_dim,activation=activa,kernel_initializer=glorot_normal(),kernel_regularizer=l2(l2_penalty))(d3)\n",
    "    d4 = Dense(num_cells, activation=\"linear\", kernel_initializer=kernel_init_func, kernel_regularizer=l2(l2_penalty), use_bias=use_bias)(d2)\n",
    "    ####################MLP################################################################################\n",
    "    centerloss_embed_layer = Embedding(num_cells, low_dim)(target_input)\n",
    "    centerloss_out = Lambda(lambda x: K.sum(K.square(x[0] - x[1][:, 0]), 1, keepdims=True), name=\"center\")([d2, centerloss_embed_layer])\n",
    "\n",
    "    softmax_out = Activation(\"softmax\", name=\"softmax\")(d4)\n",
    "\n",
    "    softmax_model = Model([SIMS_input, target_input], [softmax_out, centerloss_out])\n",
    "    softmax_model.compile(optimizer=adam(), loss=[\"categorical_crossentropy\", lambda y_true, y_pred: y_pred], loss_weights=[1, 0])\n",
    "    onehot_label = keras.utils.to_categorical(cell_idx - 1, num_cells)\n",
    "    # reset_weights(softmax_model)\n",
    "\n",
    "    history = softmax_model.fit([train_x, cell_idx - 1], [onehot_label, np.ones(shape=cell_idx.shape)], epochs=epochs, shuffle=True, batch_size=64)\n",
    "    while np.abs(history.history[\"loss\"][-1] - np.max(history.history[\"loss\"])) <= error_threshold:\n",
    "        print(\"error\")\n",
    "        reset_weights(softmax_model)\n",
    "        #         history=softmax_model.fit([train_x,cell_idx-1,dummy_input_data],[onehot_label,np.ones(shape=cell_idx.shape)],epochs=epochs,shuffle=True,batch_size=64,verbose=verbose)\n",
    "        history = softmax_model.fit([train_x, cell_idx - 1], [onehot_label, np.ones(shape=cell_idx.shape)], epochs=epochs, shuffle=True, batch_size=64)\n",
    "\n",
    "    logit_model = Model(SIMS_input, d4)\n",
    "\n",
    "    pred_logit = logit_model.predict(train_x)\n",
    "    rep_list = []\n",
    "    for t in t_list:\n",
    "        cur_representation = np.exp(pred_logit / t)\n",
    "        cur_representation = cur_representation / np.sum(cur_representation, axis=1, keepdims=True)\n",
    "        cur_representation = np.transpose(cur_representation)\n",
    "        rep_list.append(cur_representation)\n",
    "    return rep_list\n",
    "\n",
    "\n",
    "def ID(a, t=5, epochs=200):\n",
    "    matter_list = np.array(a.var_names)\n",
    "    # SIMS_id_t_list = [5,10,15,20,25,30,35,40,50]\n",
    "    SIMS_id_t_list = [t]\n",
    "    train_x_tmp = a.uns[\"train_x\"]\n",
    "    cell_idx = a.uns[\"cell_idx\"]\n",
    "    num_cells = a.shape[0]\n",
    "\n",
    "    HEG_list = matter_list\n",
    "\n",
    "    HEG_col_idx = [list(matter_list).index(HEG) for HEG in HEG_list]\n",
    "\n",
    "    netwidths = [128, 128, 128]\n",
    "\n",
    "    error_threshold = 0\n",
    "    train_x_HEG = train_x_tmp[:, HEG_col_idx]\n",
    "    train_x_preprocess = train_x_HEG\n",
    "\n",
    "    train_x_preprocess = (train_x_HEG) / np.sum(train_x_HEG, axis=1, keepdims=True)\n",
    "\n",
    "    rep_list = get_distil_rep(\n",
    "        train_x_preprocess,\n",
    "        cell_idx,\n",
    "        num_cells,\n",
    "        SIMS_id_t_list,\n",
    "        verbose=False,\n",
    "        epochs=epochs,\n",
    "        netwidths=netwidths,\n",
    "        low_dim=netwidths[2],\n",
    "        error_threshold=error_threshold,\n",
    "    )\n",
    "    a.obsm[\"ID\"] = rep_list[0]\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ID(a, epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.obsm[\"ID\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SIMLR\n",
    "import scanpy as sc\n",
    "import time\n",
    "\n",
    "input_rep = a.obsm[\"ID\"]\n",
    "c = 6\n",
    "simlr = SIMLR.SIMLR_LARGE(c, 10, 0)\n",
    "###This is how we initialize an object for SIMLR. the first input is number of rank (clusters) and the second input is number of neighbors. The third one is an binary indicator whether to use memory-saving mode. you can turn it on when the number of cells are extremely large to save some memory but with the cost of efficiency.\n",
    "\n",
    "S, F, val, ind = simlr.fit(input_rep)\n",
    "# print('Successfully Run SIMLR! SIMLR took %f seconds in total\\n' % (time.time() -         start_main))\n",
    "pred_y = simlr.fast_minibatch_kmeans(F, c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SIMLR\n",
    "import scanpy as sc\n",
    "import time\n",
    "\n",
    "\n",
    "def run_SIMLR(a, c=8, rep=\"ID\"):\n",
    "    if rep == \"ID\":\n",
    "        input_rep = a.obsm[\"ID\"]\n",
    "    elif rep == \"Mean\":\n",
    "        input_rep = a.X\n",
    "\n",
    "    start_main = time.time()\n",
    "    #     input_rep = SIMLR.helper.fast_pca(input_rep,100)\n",
    "    print(input_rep.shape, c)\n",
    "    simlr = SIMLR.SIMLR_LARGE(\n",
    "        c, 10, 0\n",
    "    )  ###This is how we initialize an object for SIMLR. the first input is number of rank (clusters) and the second input is number of neighbors. The third one is an binary indicator whether to use memory-saving mode. you can turn it on when the number of cells are extremely large to save some memory but with the cost of efficiency.\n",
    "\n",
    "    S, F, val, ind = simlr.fit(input_rep)\n",
    "    print(\"Successfully Run SIMLR! SIMLR took %f seconds in total\\n\" % (time.time() - start_main))\n",
    "    pred_y = simlr.fast_minibatch_kmeans(F, c)\n",
    "    print(\"done!\")\n",
    "    a.obs[\"SIMLR\"] = pred_y.astype(\"int\").astype(\"str\")\n",
    "    a.obs[\"SIMLR\"] = a.obs[\"SIMLR\"].astype(\"category\")\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def Cluster(a, method, cluster_param, rep=\"ID\"):\n",
    "    if method == \"SIMLR\":\n",
    "        return run_SIMLR(a, c=cluster_param, rep=rep)\n",
    "    elif method == \"Louvain\":\n",
    "        start_main = time.time()\n",
    "        #         sc.pp.neighbors(a,use_rep=rep,metric='cosine',n_neighbors=15)\n",
    "        sc.tl.louvain(a, resolution=cluster_param)\n",
    "        print(\"Successfully Run Louvain! Louvain took %f seconds in total\\n\" % (time.time() - start_main))\n",
    "\n",
    "    elif method == \"Leiden\":\n",
    "        start_main = time.time()\n",
    "        #         sc.pp.neighbors(a,use_rep=rep,metric='cosine',n_neighbors=15)\n",
    "        sc.tl.leiden(a, resolution=cluster_param)\n",
    "        print(\"Successfully Run Leiden! Leiden took %f seconds in total\\n\" % (time.time() - start_main))\n",
    "\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Umap(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(a, color=\"SIMLR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Cluster(a, method=\"SIMLR\", cluster_param=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m_sc(a_use, pval_thre, cls, method=\"pval_topk\"):\n",
    "    #     a_use = a_m_hepa\n",
    "    #     pval_thre = 0.01\n",
    "    sc.tl.rank_genes_groups(a_use, n_genes=a_use.shape[1], groupby=cls)\n",
    "\n",
    "    rec2mat_fun = lambda rec: np.vstack([np.array(list(m)) for m in rec])\n",
    "    name_mat = a_use.uns[\"rank_genes_groups\"][\"names\"]\n",
    "    pval_mat = a_use.uns[\"rank_genes_groups\"][\"pvals_adj\"]\n",
    "    score_mat = a_use.uns[\"rank_genes_groups\"][\"scores\"]\n",
    "\n",
    "    name_mat = rec2mat_fun(name_mat)\n",
    "    pval_mat = rec2mat_fun(pval_mat)\n",
    "    score_mat = rec2mat_fun(score_mat)\n",
    "    n_cls = name_mat.shape[1]\n",
    "\n",
    "    rst_list = []\n",
    "    for i in range(n_cls):\n",
    "        if method == \"pval_thre\":\n",
    "            cur_idx = pval_mat[:, i] <= pval_thre\n",
    "        elif method == \"pval_topk\":\n",
    "            kth_p = np.sort(pval_mat[:, i])[pval_thre]\n",
    "            cur_idx = pval_mat[:, i] <= kth_p\n",
    "        elif method == \"score_thre\":\n",
    "            cur_idx = score_mat[:, i] >= pval_thre\n",
    "        elif method == \"score_topk\":\n",
    "            kth_score = np.flip(np.sort(score_mat[:, i]))[pval_thre]\n",
    "            cur_idx = score_mat[:, i] >= kth_score\n",
    "\n",
    "        cur_m = name_mat[cur_idx, i]\n",
    "        rst_list.append(cur_m)\n",
    "        print(cur_m.shape[0])\n",
    "    a_use.uns[cls + \"_mz\"] = rst_list\n",
    "    return a_use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = get_m_sc(a, 10, cls=\"SIMLR\", method=\"pval_topk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import *\n",
    "\n",
    "\n",
    "def get_dist_mat_emd(a, method=\"emd\"):\n",
    "\n",
    "    train_x = a.uns[\"train_x\"]\n",
    "    num_cells = a.shape[0]\n",
    "    num_features = a.shape[1]\n",
    "    cell_idx = a.uns[\"cell_idx\"]\n",
    "    cell_pixel_dict = {}\n",
    "    pixel_count = []\n",
    "    for i in range(num_cells):\n",
    "        cur_pixels = train_x[cell_idx == i + 1, :]\n",
    "        cell_pixel_dict[i] = cur_pixels\n",
    "        pixel_count.append(cur_pixels.shape[0])\n",
    "\n",
    "    dist_mat = np.zeros(shape=(num_features, num_cells, num_cells))\n",
    "    for k in range(num_features):\n",
    "        if k % 10 == 0:\n",
    "            print(k)\n",
    "        #         print(k)\n",
    "        if method == \"emd\":\n",
    "            for i in range(num_cells):\n",
    "                for j in range(num_cells):\n",
    "                    cur_dist = wasserstein_distance(cell_pixel_dict[i][:, k], cell_pixel_dict[j][:, k])\n",
    "                    #             cur_dist = euclidean(np.mean(cell_pixel_dict[i][:,k]),np.mean(cell_pixel_dict[j][:,k]))\n",
    "                    #             print(k,i,j,cur_dist)\n",
    "                    #                     cur_dist = np.mean(cell_pixel_dict[i][:,k])-np.mean(cell_pixel_dict[j][:,k])\n",
    "                    dist_mat[k, i, j] = cur_dist\n",
    "        elif method == \"euclidean\":\n",
    "            cur_dist_mat = squareform(pdist(a.X[:, k][:, None]))\n",
    "            dist_mat[k, :, :] = cur_dist_mat\n",
    "    a.uns[\"feature_wise_distmat\"] = dist_mat\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_wbr(dist_mat, pred_list):\n",
    "    wbr_list = []\n",
    "    for i in range(dist_mat.shape[0]):\n",
    "        cur_dist_mat = dist_mat[i, :, :]\n",
    "        within_sum_1 = np.sum(dist_mat[i, :, :][pred_list == 1, :][:, pred_list == 1])\n",
    "        within_sum_0 = np.sum(dist_mat[i, :, :][pred_list == 0, :][:, pred_list == 0])\n",
    "        between_sum_1 = np.sum(dist_mat[i, :, :][pred_list == 1, :][:, pred_list == 0])\n",
    "        between_sum_0 = np.sum(dist_mat[i, :, :][pred_list == 0, :][:, pred_list == 1])\n",
    "        wbr = (within_sum_1 + within_sum_0) / (between_sum_1 + between_sum_0)\n",
    "        wbr_list.append(wbr)\n",
    "    return np.array(wbr_list)\n",
    "\n",
    "\n",
    "def get_wbr_mat(a_use, cls):\n",
    "    #     a_use = a\n",
    "    # #     wbr_thre = 10\n",
    "    #     cls = 'SIMLR'\n",
    "    #     method='topk'\n",
    "    dist_mat = a_use.uns[\"feature_wise_distmat\"]\n",
    "    unique_labels = np.unique(a_use.obs[cls])\n",
    "    wbr_mat = np.zeros(shape=(a_use.shape[1], unique_labels.shape[0]))\n",
    "\n",
    "    for i in range(wbr_mat.shape[1]):\n",
    "        cur_label = unique_labels[i]\n",
    "        cur_pred = a_use.obs[cls].copy().astype(\"str\")\n",
    "        cur_pred[cur_pred != cur_label] = -1\n",
    "        cur_pred[cur_pred == cur_label] = 0\n",
    "        cur_pred = -cur_pred\n",
    "        cur_wbr_list = get_wbr(dist_mat, cur_pred)\n",
    "        wbr_mat[:, i] = cur_wbr_list\n",
    "\n",
    "    a_use.uns[\"rank_genes_groups_emd\"] = {\"scores\": wbr_mat}\n",
    "    return a_use\n",
    "\n",
    "\n",
    "def get_m_emd(a_use, thre, cls, method=\"topk\", alg=\"emd\"):\n",
    "    a_use = get_dist_mat_emd(a_use, method=alg)\n",
    "    a_use = get_wbr_mat(a_use, cls)\n",
    "\n",
    "    score_mat = a_use.uns[\"rank_genes_groups_emd\"][\"scores\"]\n",
    "    n_cls = score_mat.shape[1]\n",
    "    name_list = a_use.var_names\n",
    "    rst_list = []\n",
    "    for i in range(n_cls):\n",
    "        if method == \"thre\":\n",
    "            cur_idx = score_mat[:, i] <= thre\n",
    "        elif method == \"topk\":\n",
    "            kth_p = np.sort(score_mat[:, i])[thre]\n",
    "            cur_idx = score_mat[:, i] <= kth_p\n",
    "\n",
    "        cur_m = np.array(name_list[cur_idx])\n",
    "        rst_list.append(cur_m)\n",
    "        print(cur_m.shape[0])\n",
    "    a_use.uns[cls + \"_mz_emd\"] = rst_list\n",
    "    return a_use\n",
    "\n",
    "\n",
    "# def Diff(a_use,cls,method='sc'):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.uns[\"rank_genes_groups\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_use = get_m_emd(a, 20, \"SIMLR\", method=\"topk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_use.uns[\"SIMLR_mz_emd\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "\n",
    "def View(a, method=\"Umap\"):\n",
    "\n",
    "    data_all = a.uns[\"IMS\"]\n",
    "    pseudo_count = 1\n",
    "    data_all_norm = (data_all + pseudo_count) / (np.percentile(data_all, 50, axis=1, keepdims=True) + pseudo_count)\n",
    "    data_all_norm = MinMaxScaler().fit_transform(data_all_norm)\n",
    "    if method == \"Umap\":\n",
    "        fg_umap = umap.UMAP(n_components=3, n_neighbors=50).fit_transform(data_all_norm)\n",
    "        a.uns[\"IMS_Umap\"] = fg_umap\n",
    "    elif method == \"Tsne\":\n",
    "        fg_tsne = TSNE(n_components=3).fit_transform(data_all_norm)\n",
    "        a.uns[\"IMS_Tsne\"] = fg_tsne\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = View(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_SIMS_view(a.uns[\"IMS_Umap\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.IMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_mz_img(a, mz):\n",
    "    m_list = a.var_names\n",
    "    img_flatten = a.uns[\"IMS\"][:, m_list == mz]\n",
    "    img_square = img_flatten.reshape(256, 256)\n",
    "    return img_square\n",
    "\n",
    "\n",
    "def IMS(a, mz):\n",
    "    mz_img = get_mz_img(a, mz)\n",
    "    plt.imshow(mz_img)\n",
    "    plt.title(mz + \" m/z\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMS(a, \"59.3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.Cut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(a, method=\"dot\"):\n",
    "    if method == \"dot\":\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "        sc.pl.embedding(a, basis=\"spatial\", ax=ax)\n",
    "    elif method == \"mask\":\n",
    "        pred_y = np.ones(shape=(a.shape[0],))\n",
    "        cmp = [\"w\"]\n",
    "        plot_label_image(a, pred_y, cmp, save=None, mask=None, figsize=(5, 5), anno=False, ifshow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cut(a, method=\"dot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cut(a, method=\"mask\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID(a):\n",
    "    sc.pl.umap(a, color=\"SIMLR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster(a, cls, groups, method=\"mask\"):\n",
    "    #     groups are list of label index\n",
    "    if method == \"mask\":\n",
    "        plot_label_image(a, a.obs[cls], a.uns[cls + \"_colors\"], mask=groups, save=None)\n",
    "    elif method == \"dot\":\n",
    "        unique_labels = np.unique(a.obs[cls])\n",
    "        sc.pl.embedding(a, basis=\"spatial\", color=cls, groups=list(unique_labels[groups]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster(a, \"SIMLR\", [2, 4], \"mask\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster(a, \"SIMLR\", [2, 4], \"dot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.Diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_use.uns[\"SIMLR_mz_emd\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(a, cls, method=\"SIMLR_mz_emd\", show_gene_labels=False):\n",
    "    mzs = np.hstack(a.uns[\"SIMLR_mz_emd\"])\n",
    "    cls = \"SIMLR\"\n",
    "    sc.pl.heatmap(\n",
    "        a, var_names=mzs, groupby=cls, standard_scale=\"var\", cmap=heatmap_cmp, dendrogram=False, save=None, swap_axes=True, show_gene_labels=show_gene_labels\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diff(a_use, \"SIMLR\", show_gene_labels=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_SIMS_view(fg_umap, save=None):\n",
    "    # fg_umap = a_concat[a_concat.obs['batch']=='0'].obsm['X_pca'][:,0:3]\n",
    "\n",
    "    fg_umap_norm = MinMaxScaler().fit_transform(fg_umap)\n",
    "    fg_umap_norm[:, 0] = MinMaxScaler(feature_range=(0, 100)).fit_transform(fg_umap_norm[:, 0][:, None])[:, 0]\n",
    "    fg_umap_norm[:, 1] = MinMaxScaler(feature_range=(-128, 127)).fit_transform(fg_umap_norm[:, 1][:, None])[:, 0]\n",
    "    fg_umap_norm[:, 2] = MinMaxScaler(feature_range=(-128, 127)).fit_transform(fg_umap_norm[:, 2][:, None])[:, 0]\n",
    "    #     fg_umap_norm[:,2] = MinMaxScaler(feature_range=(-128, 50)).fit_transform(fg_umap_norm[:,2][:,None])[:,0]\n",
    "\n",
    "    data_rgb = fg_umap_norm\n",
    "\n",
    "    data_rgb_img = data_rgb.reshape(256, 256, 3).astype(\"float64\")\n",
    "    data_rgb_img = lab2rgb(data_rgb_img)\n",
    "    sns.set(style=\"white\")\n",
    "    sns.set_color_codes(\"deep\")\n",
    "    # sns.set\n",
    "    # cur_save_file = '{0}tsnemap_img_thres134_{1}_{2}.png'.format(save_path,str(bg_threshold),time_str)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(data_rgb_img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if save is not None:\n",
    "        plt.savefig(save, transparent=False, format=\"png\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def View(a, method=\"Umap\"):\n",
    "    plot_SIMS_view(a.uns[\"IMS_\" + method])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaden)",
   "language": "python",
   "name": "scaden"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
